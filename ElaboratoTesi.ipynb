{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"04c63f7bfe3046a9ba59d04d69b62f0d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b6ed5534226f45bbb7f31fe9c4c8a44f","IPY_MODEL_baf3826a029f42468980b704bc33385f","IPY_MODEL_ffb25caa44174f0cb6616a5c304fb744"],"layout":"IPY_MODEL_7f4d0b63a3b54e1a8515d02de5543075"}},"b6ed5534226f45bbb7f31fe9c4c8a44f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05cfae46c6ba4600882af79f8b05d026","placeholder":"​","style":"IPY_MODEL_4db5abfa8ec041bea6c3b016bade45cd","value":"open_clip_pytorch_model.bin: 100%"}},"baf3826a029f42468980b704bc33385f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b10c9d318bc8480fb2afb14e6a0bfd7d","max":605219813,"min":0,"orientation":"horizontal","style":"IPY_MODEL_025361e448b34839aefebbf6ec72152a","value":605219813}},"ffb25caa44174f0cb6616a5c304fb744":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37c290bd0eb24fc8a5b4eb29d1fa7603","placeholder":"​","style":"IPY_MODEL_9f60d4f8beac409ca1fbacde5e8d77bf","value":" 605M/605M [00:05&lt;00:00, 90.9MB/s]"}},"7f4d0b63a3b54e1a8515d02de5543075":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05cfae46c6ba4600882af79f8b05d026":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4db5abfa8ec041bea6c3b016bade45cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b10c9d318bc8480fb2afb14e6a0bfd7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"025361e448b34839aefebbf6ec72152a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"37c290bd0eb24fc8a5b4eb29d1fa7603":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f60d4f8beac409ca1fbacde5e8d77bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7QlUkb96Mt3_"},"outputs":[],"source":["!pip install transformers requests\n","!pip install open_clip_torch\n","!pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","source":["import io\n","from io import BytesIO\n","import torch\n","from PIL import Image\n","import requests\n","from transformers import CLIPProcessor, CLIPModel\n","import open_clip\n","import os\n","import skimage\n","import IPython.display\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import shutil\n","import pickle\n","from collections import OrderedDict\n","import torch.nn as nn\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch.utils.data import Dataset\n","import clip"],"metadata":{"id":"UUKNyI6bMvWn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gcwcRYSdCRy-","executionInfo":{"status":"ok","timestamp":1700729821200,"user_tz":-60,"elapsed":29357,"user":{"displayName":"leonardo notari","userId":"06802545493190193317"}},"outputId":"b94acbaa-0c2c-44b8-b0ce-0c43f45220d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["**CARICA E CREA DATASET PER I VARI TEST**"],"metadata":{"id":"bdqVZg05NdAe"}},{"cell_type":"code","source":["labels = [\"sadness\", \"awe\", \"amusement\", \"contentment\", \"excitement\", \"fear\", \"disgust\", \"anger\"]\n","X_images = []\n","X_utterances = []\n","Y_images = []\n","lbl_emotions_img = []\n","\n","X_texts = []\n","Y_texts = []\n","lbl_emotions_txt = []\n","\n","file_path = '/content/drive/My Drive/dataset_tesi/index_images.pkl'\n","with open(file_path, 'rb') as file:\n","    index_images = pickle.load(file)\n","X_images = index_images\n","\n","'''\n","j = 0\n","folder_path = \"/content/drive/My Drive/paintings_images/\"\n","for i in index_images[:1000]:\n","    j += 1\n","    print(j)\n","    filename = str(i) + '.jpg'\n","    image_path = os.path.join(folder_path, filename)\n","    image = Image.open(image_path)\n","    X_images.append(image)\n","'''\n","\n","file_path = '/content/drive/My Drive/dataset_tesi/utterances.pkl'\n","with open(file_path, 'rb') as file:\n","    X_utterances = pickle.load(file)\n","\n","file_path = '/content/drive/My Drive/dataset_tesi/emotions.pkl'\n","with open(file_path, 'rb') as file:\n","    Y_images = pickle.load(file)\n","\n","for i in Y_images:\n","    lbl_emotions_img.append(labels.index(i))\n","\n","\n","file_path = '/content/drive/My Drive/dataset_tesi/texts_map.pkl'\n","with open(file_path, 'rb') as file:\n","    texts_map = pickle.load(file)\n","\n","for i in texts_map:\n","    X_texts.append(i[2])\n","    Y_texts.append(i[3])\n","\n","for i in Y_texts:\n","    lbl_emotions_txt.append(labels.index(i))\n","\n"],"metadata":{"id":"yl4n2QzrMz1Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Counter\n","\n","X_texts_tr = []\n","Y_texts_tr = []\n","lbl_emotions_txt_tr = []\n","X_texts_te = []\n","Y_texts_te = []\n","lbl_emotions_txt_te = []\n","for i in range(len(X_texts)):\n","  if lbl_emotions_txt_tr.count(lbl_emotions_txt[i]) < 6250:\n","    X_texts_tr.append(X_texts[i])\n","    Y_texts_tr.append(Y_texts[i])\n","    lbl_emotions_txt_tr.append(lbl_emotions_txt[i])\n","  else:\n","    if lbl_emotions_txt_te.count(lbl_emotions_txt[i]) < 250:\n","      X_texts_te.append(X_texts[i])\n","      Y_texts_te.append(Y_texts[i])\n","      lbl_emotions_txt_te.append(lbl_emotions_txt[i])\n","\n"],"metadata":{"id":"vnzZh9DBUXOW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dist = [0,0,0,0,0,0,0,0]\n","\n","for i in lbl_emotions_txt_te:\n","  dist[i]+=1\n","print(dist)\n","somma = 0\n","for  i in dist:\n","  somma += i\n","  print(i/len(lbl_emotions_txt))\n","print(somma)"],"metadata":{"id":"-U1V8a2IZ4E6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(\"A\")\n","else:\n","    device = 'cpu'\n","    print(\"B\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"00d8OnFbXQZM","executionInfo":{"status":"ok","timestamp":1700730445074,"user_tz":-60,"elapsed":410,"user":{"displayName":"leonardo notari","userId":"06802545493190193317"}},"outputId":"7f14e622-998e-439e-91b3-c8d40642862a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["A\n"]}]},{"cell_type":"markdown","source":["**CARICA IL MODELLO**"],"metadata":{"id":"rFLaO3YRN5B5"}},{"cell_type":"code","source":["model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n","tokenizer = open_clip.get_tokenizer('ViT-B-32')"],"metadata":{"id":"DyVyiEGsNDxk","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["04c63f7bfe3046a9ba59d04d69b62f0d","b6ed5534226f45bbb7f31fe9c4c8a44f","baf3826a029f42468980b704bc33385f","ffb25caa44174f0cb6616a5c304fb744","7f4d0b63a3b54e1a8515d02de5543075","05cfae46c6ba4600882af79f8b05d026","4db5abfa8ec041bea6c3b016bade45cd","b10c9d318bc8480fb2afb14e6a0bfd7d","025361e448b34839aefebbf6ec72152a","37c290bd0eb24fc8a5b4eb29d1fa7603","9f60d4f8beac409ca1fbacde5e8d77bf"]},"executionInfo":{"status":"ok","timestamp":1700730459372,"user_tz":-60,"elapsed":8348,"user":{"displayName":"leonardo notari","userId":"06802545493190193317"}},"outputId":"841f223c-0e60-41cb-c942-7fd66a1ca592"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["open_clip_pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04c63f7bfe3046a9ba59d04d69b62f0d"}},"metadata":{}}]},{"cell_type":"markdown","source":["**CLASSE PER IL DATALOADER**"],"metadata":{"id":"BjYxZgzkN9sH"}},{"cell_type":"code","source":["class images_emotions_dataset():\n","    def __init__(self, images, emotions, labels):\n","        self.images = images\n","        self.emotions = tokenizer(emotions)\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        folder_path = \"/content/drive/My Drive/paintings_images/\"\n","        filename = str(self.images[idx]) + '.jpg'\n","        image_path = os.path.join(folder_path, filename)\n","        image = Image.open(image_path)\n","        image = preprocess(image)\n","        label = self.labels[idx]\n","        emotion = self.emotions[idx]\n","        return image, emotion, label"],"metadata":{"id":"sfWS7y40NEfj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**FINETUNING SULLE IMMAGINI**"],"metadata":{"id":"ASPpMGXzOCp_"}},{"cell_type":"code","source":["model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')"],"metadata":{"id":"ciHyslHB4667"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","batch_size = 64\n","k = 50\n","Xtr = X_images[:k]\n","Ytr = Y_images[:k]\n","lbl_images = lbl_emotions_img[:k]\n","train_dataset = images_emotions_dataset(Xtr, Ytr, lbl_images)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","model.train()\n","model.to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.000003,eps=1e-6,weight_decay=0.001)\n","loss = nn.CrossEntropyLoss()\n","\n","num_epochs = 1\n","for epoch in range(num_epochs):\n","    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n","    for batch in pbar:\n","        optimizer.zero_grad()\n","        images, emotions, lbl_images = batch\n","        images = images.to(device)\n","        emotions = emotions.to(device)\n","        lbl_images = lbl_images.to(device)\n","        outputs = model(images, emotions)\n","        logits_per_images = outputs[0]\n","        logits_per_emotions = outputs[1]\n","        ground_truth = torch.tensor(lbl_images, dtype=torch.long)\n","        total_loss = (loss(logits_per_images, ground_truth) + loss(logits_per_emotions, ground_truth))/2\n","        total_loss.backward()\n","        optimizer.step()\n","        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")\n","        torch.cuda.empty_cache()"],"metadata":{"id":"ZosfU5T9NHYQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700157907926,"user_tz":-60,"elapsed":5492,"user":{"displayName":"Leonardo Notari","userId":"10004708555133482522"}},"outputId":"94c8b141-bb93-48a3-86ea-646074873f32"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/1 [00:00<?, ?it/s]<ipython-input-14-c821b730e657>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  ground_truth = torch.tensor(lbl_images, dtype=torch.long)\n","Epoch 0/1, Loss: 5.4674: 100%|██████████| 1/1 [00:05<00:00,  5.19s/it]\n"]}]},{"cell_type":"code","source":["model.to(device)\n","model.eval()\n","count = 0\n","#X = X_images[:1000]\n","#Y = Y_images[:1000]\n","l = len(X_images)\n","X = X_images[l-100:l]\n","Y = Y_images[l-100:l]\n","lbl = labels\n","for i in range(len(Y)):\n","    image = X[i]\n","    image = preprocess(image).unsqueeze(0)\n","    text = tokenizer(lbl)\n","    with torch.no_grad(), torch.cuda.amp.autocast():\n","      text = text.to(device)\n","      image = image.to(device)\n","      outputs = model(image, text)\n","      logits_per_image = outputs[0]\n","      logits_per_text = outputs[1]\n","      probs = (100.0 * logits_per_image @ logits_per_text.T).softmax(dim=-1)\n","      probs = probs.to('cpu')[0]\n","    index = np.where(probs == max(probs))\n","    if Y[i] == lbl[index[0][0]]:\n","        count += 1\n","print(count)"],"metadata":{"id":"W8DzQjqxNTRr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700134868855,"user_tz":-60,"elapsed":10370,"user":{"displayName":"Leonardo Notari","userId":"10004708555133482522"}},"outputId":"1b8e417f-f201-494f-e4b3-2af5b872c27e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["57\n"]}]},{"cell_type":"markdown","source":["**CLASS PER IL DATALOADER DI TESTI**"],"metadata":{"id":"LFx4LPlQ-kqN"}},{"cell_type":"code","source":["class texts_emotions_dataset():\n","    def __init__(self, texts, emotions, labels):\n","        self.title  = tokenizer(texts)\n","        self.emotions  = tokenizer(emotions)\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.title)\n","\n","    def __getitem__(self, idx):\n","        title = self.title[idx]\n","        emotion = self.emotions[idx]\n","        label = self.labels[idx]\n","        return title, emotion, label"],"metadata":{"id":"WLOtadtD-Zjo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**FINETUNING SUI TESTI**"],"metadata":{"id":"tkRBX2MNOIir"}},{"cell_type":"code","source":["model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')"],"metadata":{"id":"J4occdJB4hvp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 128\n","Xtr = X_texts_tr\n","Ytr = Y_texts_tr\n","\n","lbl_tr = lbl_emotions_txt_tr\n","train_dataset = texts_emotions_dataset(Xtr, Ytr, lbl_tr)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","model.to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.000005,eps=1e-6,weight_decay=0.01)\n","loss = nn.CrossEntropyLoss()\n","\n","num_epochs = 4\n","for epoch in range(num_epochs):\n","    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n","    for batch in pbar:\n","        optimizer.zero_grad()\n","        texts, emotions, lbls = batch\n","        texts = texts.to(device)\n","        emotions = emotions.to(device)\n","        lbls = lbls.to(device)\n","        logits_per_texts = model.encode_text(texts)\n","        logits_per_emotions = model.encode_text(texts)\n","        ground_truth = torch.tensor(lbls, dtype=torch.long)\n","        total_loss = (loss(logits_per_texts, ground_truth) + loss(logits_per_emotions, ground_truth))/2\n","        total_loss.backward()\n","        optimizer.step()\n","        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")"],"metadata":{"id":"D7B9KN0RNQb5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for e in [0.000005, 0.000007, 0.00001]:\n","    model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n","    batch_size = 128\n","    Xtr = X_texts_tr\n","    Ytr = Y_texts_tr\n","\n","    lbl_tr = lbl_emotions_txt_tr\n","    train_dataset = texts_emotions_dataset(Xtr, Ytr, lbl_tr)\n","    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","    model.to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=e,eps=1e-6,weight_decay=0.01)\n","    loss = nn.CrossEntropyLoss()\n","\n","    num_epochs = 4\n","    for epoch in range(num_epochs):\n","        pbar = tqdm(train_dataloader, total=len(train_dataloader))\n","        for batch in pbar:\n","            optimizer.zero_grad()\n","            texts, emotions, lbls = batch\n","            texts = texts.to(device)\n","            emotions = emotions.to(device)\n","            lbls = lbls.to(device)\n","            logits_per_texts = model.encode_text(texts)\n","            logits_per_emotions = model.encode_text(texts)\n","            ground_truth = torch.tensor(lbls, dtype=torch.long)\n","            total_loss = (loss(logits_per_texts, ground_truth) + loss(logits_per_emotions, ground_truth))/2\n","            total_loss.backward()\n","            optimizer.step()\n","            pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")\n","    count = 0\n","    X = X_texts_te\n","    Y = Y_texts_te\n","    lbl = labels\n","    for i in range(len(Y)):\n","        text = tokenizer(X[i])\n","        with torch.no_grad(), torch.cuda.amp.autocast():\n","          emotions = clip.tokenize(lbl)\n","          text = text.to(device)\n","          emotions = emotions.to(device)\n","          emotions_features = model.encode_text(emotions)\n","          text_features = model.encode_text(text)\n","          emotions_features /= emotions_features.norm(dim=-1, keepdim=True)\n","          text_features /= text_features.norm(dim=-1, keepdim=True)\n","          probs = (100.0 * text_features @ emotions_features.T).softmax(dim=-1)\n","        probs = probs.to('cpu')[0]\n","        index = np.where(probs == max(probs))\n","        if Y[i] == lbl[index[0][0]]:\n","            count += 1\n","    print(count)\n","    with open('/content/drive/My Drive/risultati1.txt', 'a') as file:\n","        file.write(str(e)+\" risultato: \"+str(count)+\"\\n\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":552},"id":"lrrp4jcZczja","executionInfo":{"status":"error","timestamp":1700732715124,"user_tz":-60,"elapsed":15280,"user":{"displayName":"leonardo notari","userId":"06802545493190193317"}},"outputId":"bf1dd8fb-9bf4-494e-a1ca-13a973b52654"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/391 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"OutOfMemoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-993a29352a00>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mlbls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlbls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mlogits_per_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mlogits_per_emotions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mground_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlbls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_per_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_per_emotions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/open_clip/model.py\u001b[0m in \u001b[0;36mencode_text\u001b[0;34m(self, text, normalize)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# NLD -> LND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# LND -> NLD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch_size, n_ctx, transformer.width]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/open_clip/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/open_clip/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q_x, k_x, v_x, attn_mask)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mv_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ln_1_kv\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mv_x\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mls_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mls_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/open_clip/transformer.py\u001b[0m in \u001b[0;36mattention\u001b[0;34m(self, q_x, k_x, v_x, attn_mask)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mattn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mattn_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         return self.attn(\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0mq_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         )[0]\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 is_causal=is_causal)\n\u001b[1;32m   1240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1242\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5298\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_separate_proj_weight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5299\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0min_proj_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_separate_proj_weight is False but in_proj_weight is None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5300\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_in_projection_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5301\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5302\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_proj_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_separate_proj_weight is True but q_proj_weight is None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4822\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4823\u001b[0m             \u001b[0;31m# self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4824\u001b[0;31m             \u001b[0mproj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4825\u001b[0m             \u001b[0;31m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4826\u001b[0m             \u001b[0mproj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 30.81 MiB is free. Process 4635 has 14.71 GiB memory in use. Of the allocated memory 13.46 GiB is allocated by PyTorch, and 430.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"code","source":["torch.cuda.empty_cache()\n","import gc\n","\n","\n","gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1UCkceYEb3xU","executionInfo":{"status":"ok","timestamp":1700665661734,"user_tz":-60,"elapsed":1089,"user":{"displayName":"Leonardo Notari","userId":"10004708555133482522"}},"outputId":"114a29d0-2688-4274-e3d0-fd63749d58c1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["model.to(device)\n","count = 0\n","X = X_texts_te\n","Y = Y_texts_te\n","lbl = labels\n","for i in range(len(Y)):\n","    text = tokenizer(X[i])\n","    with torch.no_grad(), torch.cuda.amp.autocast():\n","      emotions = clip.tokenize(lbl)\n","      text = text.to(device)\n","      emotions = emotions.to(device)\n","      emotions_features = model.encode_text(emotions)\n","      text_features = model.encode_text(text)\n","      emotions_features /= emotions_features.norm(dim=-1, keepdim=True)\n","      text_features /= text_features.norm(dim=-1, keepdim=True)\n","      probs = (100.0 * text_features @ emotions_features.T).softmax(dim=-1)\n","    probs = probs.to('cpu')[0]\n","    index = np.where(probs == max(probs))\n","    if Y[i] == lbl[index[0][0]]:\n","        count += 1\n","print(count)"],"metadata":{"id":"riUmjhcm-PVf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700696958237,"user_tz":-60,"elapsed":52555,"user":{"displayName":"leonardo notari","userId":"06802545493190193317"}},"outputId":"836a9baa-efb7-43c7-f18b-a16f71da725f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1115\n"]}]},{"cell_type":"markdown","source":["**MLP PER EMBEDDINGS COMBINATI**"],"metadata":{"id":"Us67DcOWOQdR"}},{"cell_type":"code","source":["class images_utterances_dataset():\n","    def __init__(self, images, texts, labels):\n","        self.images = images\n","        self.title  = tokenizer(texts)\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.title)\n","\n","    def __getitem__(self, idx):\n","        folder_path = \"/content/drive/My Drive/paintings_images/\"\n","        filename = str(self.images[idx]) + '.jpg'\n","        image_path = os.path.join(folder_path, filename)\n","        image = Image.open(image_path)\n","        image = preprocess(image)\n","        title = self.title[idx]\n","        label = self.labels[idx]\n","        return image, title, label"],"metadata":{"id":"a9GIgMiUxPRS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')"],"metadata":{"id":"HYKE-NC94-IJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_dim = 1024\n","hidden_dim = 256\n","output_dim = len(labels)\n","\n","mlp_model = nn.Sequential(\n","    nn.Linear(input_dim, 512),\n","    nn.ReLU(),\n","    nn.Linear(512, 256),\n","    nn.ReLU(),\n","    nn.Linear(256, 128),\n","    nn.ReLU(),\n","    nn.Linear(128, 64),\n","    nn.ReLU(),\n","    nn.Linear(64, 32),\n","    nn.ReLU(),\n","    nn.Linear(32, output_dim)\n",")\n","\n","k = 1000\n","optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.0005, eps=1e-6, weight_decay=0.02)\n","loss = nn.CrossEntropyLoss()\n","Xtr = X_images[:k]\n","Xtr_texts = X_utterances[:k]\n","lbl_tr = lbl_emotions_img[:k]\n","batch_size = 32\n","dataset = images_utterances_dataset(Xtr, Xtr_texts, lbl_tr)\n","train_dataloader = DataLoader(dataset, batch_size=batch_size)\n","mlp_model.train()\n","model.train()\n","num_epochs = 10\n","model.to(device)\n","mlp_model.to(device)\n","for epoch in range(num_epochs):\n","    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n","    for batch in pbar:\n","        optimizer.zero_grad()\n","        images, texts, ys = batch\n","        images = images.to(device)\n","        texts = texts.to(device)\n","        image_features = model.encode_image(images)\n","        text_features = model.encode_text(texts)\n","        combined_features = torch.cat((text_features, image_features), dim=1)\n","        outputs = mlp_model(combined_features)\n","        ground_truth = torch.tensor(ys, dtype=torch.long, device=device)\n","        total_loss = loss(outputs, ground_truth)\n","        total_loss.backward()\n","        optimizer.step()\n","        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")"],"metadata":{"id":"M-mIUE8vNWNw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**TEST MLP**"],"metadata":{"id":"tpnLbISKOWIj"}},{"cell_type":"code","source":["X = X_images[1000:1500]\n","Y = Y_images[1000:1500]\n","Xtexts = X_utterances[1000:1500]\n","mlp_model.eval()\n","model.eval()\n","count = 0\n","for i in range(100):\n","    image = X[i]\n","    text = Xtexts[i]\n","\n","    folder_path = \"/content/drive/My Drive/paintings_images/\"\n","    filename = str(X_images[i]) + '.jpg'\n","    image_path = os.path.join(folder_path, filename)\n","    image = Image.open(image_path)\n","    image = preprocess(image).unsqueeze(0)\n","\n","    text = tokenizer(text)\n","    lbl = tokenizer(labels)\n","    with torch.no_grad(), torch.cuda.amp.autocast():\n","      image = image.to(device)\n","      text = text.to(device)\n","      image_features = model.encode_image(image)\n","      text_features = model.encode_text(text)\n","      combined_features = torch.cat((text_features, image_features), dim=1)\n","      outputs = mlp_model(combined_features)\n","      probs = (100.0 * outputs).softmax(dim=-1)\n","      probs = probs.to('cpu')\n","    probs = probs[0]\n","    index = np.where(probs == max(probs))\n","    print(labels[index[0][0]])\n","    if Y[i] == labels[index[0][0]]:\n","        count += 1\n","print(count)\n"],"metadata":{"id":"KREFNuaHNW_b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700158391578,"user_tz":-60,"elapsed":12879,"user":{"displayName":"Leonardo Notari","userId":"10004708555133482522"}},"outputId":"9a4d54bc-223e-4323-f71b-0ec7daa4cdb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","sadness\n","contentment\n","contentment\n","contentment\n","contentment\n","sadness\n","sadness\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","sadness\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","sadness\n","sadness\n","contentment\n","contentment\n","contentment\n","contentment\n","sadness\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","sadness\n","sadness\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","sadness\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","sadness\n","contentment\n","contentment\n","contentment\n","contentment\n","sadness\n","contentment\n","sadness\n","sadness\n","contentment\n","contentment\n","contentment\n","sadness\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","contentment\n","sadness\n","46\n"]}]},{"cell_type":"code","source":["print(count)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cW9Zcw8u0TTc","executionInfo":{"status":"ok","timestamp":1700069508772,"user_tz":-60,"elapsed":6,"user":{"displayName":"Leonardo Notari","userId":"10004708555133482522"}},"outputId":"58dcda6c-6ec3-443e-a881-441dcd7e90c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["277\n"]}]}]}